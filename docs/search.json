[
  {
    "objectID": "tidyv3.html",
    "href": "tidyv3.html",
    "title": "Tidy Tuesday - Shakespeare",
    "section": "",
    "text": "The data from this page comes from Shakespeare’s works, which are in the public domain. The datasets used here pulled their data specifically from https://shakespeare.mit.edu/ and can be found in the TidyTeusday 2024 repository, curated by Nicola Rennie.\n\n\nShow the code\n#| echo: false\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(forcats)\n\n\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/romeo_juliet.csv')\n\n\n\n\nShow the code\ntragedies &lt;- bind_rows(\n  hamlet |&gt; mutate(play = \"Hamlet\"),\n  macbeth |&gt; mutate(play = \"Macbeth\"),\n  romeo_juliet |&gt; mutate(play = \"Romeo and Juliet\")\n)\n\n\ntragedies_ghosts &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"\\\\b(ghost|witch|spirit)\\\\b\")) |&gt;\n  select(play, dialogue) |&gt;\n  mutate(supernatural = str_extract(str_to_lower(dialogue), \"\\\\b(ghost|witch|spirit)\\\\b\" )) |&gt;\n  count(play, supernatural, name = \"num_mentions\")\n\n\n\nggplot(tragedies_ghosts, aes(x = play, y = num_mentions, fill = supernatural)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Mentions of 'Ghost', 'Witch', and 'Spirit' in Shakespeare's Tragedies\",\n    x = \"Play\",\n    y = \"Number of Mentions\",\n    fill = \"\" \n    ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThis graph shows that Hamlet has a great deal many more mentions of ghosts, witches, and spirits than the other two plays. This indicates that ghosts, witches, and spirits are more relevant to the plot of Hamlet than to the other two, which makes sense because Hamlet is frequently seeing apparitions of his dead father and this is central to the plot.\n\n\nShow the code\ntragedies_exclaim &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)\\\\bHamlet!+|Macbeth!+|Romeo!+|Juliet!+\")) |&gt;\n  mutate(exclamations = str_extract(dialogue, \"(?i)\\\\bHamlet!+|Macbeth!+|Romeo!+|Juliet!+\")) |&gt;\n  count(play, exclamations, name = \"num_exclaim\") |&gt;\n  mutate(exclamations = fct_relevel(exclamations, c(\"Hamlet!\", \"Macbeth!\", \"Romeo!\", \"Juliet!\")))\n\n\n\nggplot(tragedies_exclaim, aes(x = exclamations, y = num_exclaim, fill = exclamations)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Number of times the main character's name is exclaimed\",\n    x = \"Play\",\n    y = \"Number of Mentions\",\n    fill = \"\" \n    ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThis graph shows that out of the main characters from each play, Romeo is the one who gets yelled at (or about) the most.\n\n\nShow the code\ntragedies_exclaimers &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)\\\\b(Hamlet|Macbeth|Romeo|Juliet)!+\")) |&gt; \n  mutate(\n    exclamations = str_extract(dialogue, \"(?i)\\\\b(Hamlet|Macbeth|Romeo|Juliet)!+\"),\n    exclamations = str_to_title(exclamations)  \n  ) |&gt;\n  count(play, character, exclamations, name = \"num_exclaim\") |&gt;\n  arrange(desc(num_exclaim))\n\n\ntop_exclaimers &lt;- tragedies_exclaimers |&gt; \n  slice_max(num_exclaim, n = 10)\n\nggplot(top_exclaimers, aes(\n  x = reorder(character, num_exclaim),\n  y = num_exclaim,\n  fill = exclamations\n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Characters Who Shout Names the Most\",\n    subtitle = \"Colored by which name they exclaim\",\n    x = \"Character\",\n    y = \"Number of Exclamations\",\n    fill = \"Name Shouted\"\n  ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nAnd this graph shows that not all of those times Romeo is yelled at are from Juliet’s famous line! Turns out Romeo is yelled at (or about) by a good variety of folks.\n\n\nShow the code\ntragedies_exclaimers &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)!+\")) |&gt; \n  mutate(\n    exclamations = str_extract(dialogue, \"(?i)!+\"),\n    exclamations = str_to_title(exclamations)  \n  ) |&gt;\n  count(play, character, exclamations, name = \"num_exclaim\") |&gt;\n  arrange(desc(num_exclaim))\n\n# Optionally, focus on the top 10 loudest shouters\ntop_exclaimers &lt;- tragedies_exclaimers |&gt; \n  slice_max(num_exclaim, n = 10)\n\nggplot(top_exclaimers, aes(\n  x = reorder(character, num_exclaim),\n  y = num_exclaim,\n  fill = play\n)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Criers\",\n    subtitle = \"Characters in Shakespeare's Tragedies who Exclaim the Most\",\n    x = \"Character\",\n    y = \"Number of Exclamations\",\n    fill = \"\"\n  ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nFinally, this graph tells us that despite not being yelled at nearly as much as Romeo is, Hamlet yells a lot. Perhaps this is because of all of those ghosts, spirits, and witches.\nBelow is a graph for each play showing who has the most lines. These line counts graphs tell you which character is the “main character”, which, unsurprisingly, are also the title characters. But just in case.\n\n\nShow the code\nhamlet_counts &lt;- hamlet |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Hamlet\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(hamlet_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Hamlet: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmacbeth_counts &lt;- macbeth |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Macbeth\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(macbeth_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"goldenrod\") +\n  coord_flip() +\n  labs(\n    title = \"Macbeth: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nromeo_juliet_counts &lt;- romeo_juliet |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Romeo and Juliet\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(romeo_juliet_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"maroon\") +\n  coord_flip() +\n  labs(\n    title = \"Romeo and Juliet: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Project 4",
    "section": "",
    "text": "In 2015, “OkCupid Data for Introductory Statistics and Data Science Courses” by Albert Y. Kim and Adriana Escobedo-Land was published to the academic journal Taylor & Francis online. The dataset was published with the intention of being used for teaching data science and statistics. However, by 2016, the dataset was already appearing in magazines like Fortune, with those who had encountered the dataset claiming the dataset was found to have identifiable information – that is, someone reading the dataset could identify the real person that the OkCupid profile data was scraped from. The dating profile contained extremely sensitive information, ranging from job, salary, age, and height to religion, sexual orientation, and drug use. Being able to identify someone easily from a large, easy to access dataset led to calls for review of the dataset.\nConsent\nNone of the participants consented to be part of the dataset. In the OkCupid privacy policy, users agree to their information being visible to other members on the service, and their data being shared with service providers and operating partner companies. The privacy policy repeatedly insists that privacy and consent are important to the company. It is not unreasonable for a user to read this policy and assume that their personal data would not be scraped for a public database. However, the creators of the dataset were given permission not by the individual participants but rather by the president and founders of the company. The data scraping was thus not illegal. However, Tiffany Xiao &Yifan Ma (2021) points out that legislation about technology and data are outdated, and that while privacy violations are not often illegal, they are still immoral.\nAnonymity\nThe data was found to have identifiable information. The data is from the 2010s, so the data is certainly not old enough to be anonymous. The data contained physical descriptions including height, body type, and race, as well as age, religion, occupation, sex, education, pets, children, and star sign. They also included 10 essay questions which were answered by participants. That combined with the knowledge that all of these people lived within 25 miles of San Francisco – at an exact date which was later removed – and you can definitely identify participants. The data was not at all anonymized until reviews were requested, when essay questions were randomized by row, the exact date was replaced with “from the 2010s”, and random noise was added to the age variable.\nParticipants\nGeneralizing people by sexual orientation is not an algorithm that should be created. That sort of data can be used for little good and substantial harm. To my knowledge the dataset is not currently being used for unintended purposes, but this is not a good precedent to set. Mass amounts of personal information can be easily used for systemic discrimination.\nAvailability\nThis data was made publicly available on GitHub, as well as accessible through Taylor & Francis. You can see and access the data freely online.\nKim, A. Y., & Escobedo-Land, A. (2015). OkCupid Data for Introductory Statistics and Data Science Courses. Journal of Statistics Education, 23(2). https://doi.org/10.1080/10691898.2015.11889737\nPrivacy policy – okcupid. (n.d.-a). https://okcupid-app.zendesk.com/hc/en-us/articles/22780694078491-Privacy-Policy\nXiao, T., & Ma, Y. (2021). A Letter to the Journal of Statistics and Data Science Education — A Call for Review of “OkCupid Data for Introductory Statistics and Data Science Courses” by Albert Y. Kim and Adriana Escobedo-Land. Journal of Statistics and Data Science Education, 29(2), 214–215. https://doi.org/10.1080/26939169.2021.1930812"
  },
  {
    "objectID": "datavis.html",
    "href": "datavis.html",
    "title": "Data Viz",
    "section": "",
    "text": "Here’s where you describe your plots…\n\n\n\nHere’s where you describe dashboards…"
  },
  {
    "objectID": "datavis.html#tidy-verse-1",
    "href": "datavis.html#tidy-verse-1",
    "title": "Data Viz",
    "section": "",
    "text": "Here’s where you describe your plots…"
  },
  {
    "objectID": "datavis.html#tidy-verse-2",
    "href": "datavis.html#tidy-verse-2",
    "title": "Data Viz",
    "section": "",
    "text": "Here’s where you describe dashboards…"
  },
  {
    "objectID": "TidyTuesdayD&D.html",
    "href": "TidyTuesdayD&D.html",
    "title": "Tidy Tuesday - D&D",
    "section": "",
    "text": "This is a plot I made comparing the different types of D&D monsters with their total hit points and their strength score / modifier.\n\n\nShow the code\nggplot(monsters, aes(x = hp_number, y = str, color = type)) +\n  geom_point() +\n  labs(\n    x = \"Hit Points\",\n    y = \"Strength Modifier\",\n    title = \"D&D Monsters by Type\",\n  )\n\n\n\n\n\n\n\n\n\nThis is a plot I made trying to see which of the schools of magic had the longest spell range on average. This was made complicated by the spell range being stored as character strings (eg “30 feet” or “Touch”) so I made levels. Unfortunatley this doesn’t give a great representation at how different 300 feet is from 500 feet versus 5 feet from 10 feet. Work in progress.\n\n\nShow the code\n#| echo: false\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nspells &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-17/spells.csv', show_col_types = FALSE)\n\n\n\n\nShow the code\n#range_levels &lt;- c( \"Self\", \"Touch\", \"Sight\", \"5 feet\", \"10 feet\", \"15 feet\", \"30 feet\", \"60 feet\", \"90 feet\", \"100 feet\", \"120 feet\", \"150 feet\", \"300 feet\", \"500 feet\", \"1 mile\", \"500 miles\", \"Unlimited\", \"Special\")\n\n#self, touch, sight, 5 feet, 10 feet, 15 feet, 30 feet, 60 feet, 90 feet, 100 feet, 120 feet, 150 feet, 300 feet, 500 feet, 1 mile, 500 miles\n\n#  spell duration : \n# [1] \"Instantaneous\"                   \"8 hours\"                         \"Concentration, up to 1 hour\"    \n# [4] \"24 hours\"                        \"Concentration, up to 1 minute\"   \"10 days\"    \n# [7] \"Until dispelled\"                 \"Concentration, up to 10 minutes\" \"1 hour\"  \n# [10] \"1 minute\"                        \"7 days\"                          \"Concentration, up to 8 hours\"   \n# [13] \"Special\"                         \"Up to 8 hours\"                   \"Concentration, up to 1 day\"     \n# [16] \"10 minutes\"                      \"1 day\"                           \"30 days\"\n# [19] \"Until dispelled or triggered\"    \"1 round\"                         \"Concentration, up to 2 hours\"   \n# [22] \"Up to 1 hour\"                    \"Concentration up to 10 minutes\"  \"Up to 1 minute\" \n\n\n\n\nShow the code\nspells |&gt; \n  mutate(range = fct_relevel(range, \n                             c(\"Self\", \"Touch\", \"Sight\", \"5 feet\", \"10 feet\", \"15 feet\", \"30 feet\", \"60 feet\", \"90 feet\", \"100 feet\", \"120 feet\", \"150 feet\", \"300 feet\", \"500 feet\", \"1 mile\", \"500 miles\", \"Unlimited\", \"Special\"))) |&gt; \n  mutate(school = fct_reorder(school, as.numeric(range), .fun = \"mean\", .desc = TRUE)) |&gt; \n  ggplot(aes(x = school, y = range, color = school)) +\n  geom_point() +\n  labs(\n    x = \"School of Magic\",\n    y = \"Range of Spell\",\n    title = \"D&D Spell Range by School\",\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n#  spell duration : \n# [1] \"Instantaneous\"                   \"8 hours\"                         \"Concentration, up to 1 hour\"    \n# [4] \"24 hours\"                        \"Concentration, up to 1 minute\"   \"10 days\"    \n# [7] \"Until dispelled\"                 \"Concentration, up to 10 minutes\" \"1 hour\"  \n# [10] \"1 minute\"                        \"7 days\"                          \"Concentration, up to 8 hours\"   \n# [13] \"Special\"                         \"Up to 8 hours\"                   \"Concentration, up to 1 day\"     \n# [16] \"10 minutes\"                      \"1 day\"                           \"30 days\"\n# [19] \"Until dispelled or triggered\"    \"1 round\"                         \"Concentration, up to 2 hours\"   \n# [22] \"Up to 1 hour\"                    \"Concentration up to 10 minutes\"  \"Up to 1 minute\" \n\n\ncon_spells &lt;- spells |&gt;\n  filter(grepl(\"Concentration\", duration, ignore.case = TRUE)) \n\n\n\ncon_spells |&gt; \n  mutate(duration = fct_relevel(duration, \n                                c(\"Concentration, up to 1 minute\", \"Concentration, up to 10 minutes\", \"Concentration, up to 1 hour\", \"Concentration, up to 2 hours\", \"Concentration, up to 8 hours\", \"Concentration, up to 1 day\"))) |&gt; \n  filter(!grepl(\"Concentration up to 10 minutes\", duration, ignore.case = TRUE)) |&gt;\n  ggplot(aes(x = school, y = duration, color = school)) +\n  geom_point() +\n  labs(\n    x = \"School of Magic\",\n    y = \"Duration of Spell\",\n    title = \"Concentration Spells by School\",\n  )\n\n\n\n\n\n\n\n\n\nTidyverse Link: https://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-12-17\nDataset Citation:\nHarmon J (2025). dddata: Data from the Dungeons and Dragons System Reference Document. R package version 0.0.0.9000, https://github.com/jonthegeek/dddata.\nOriginal Data Source:\n“Dungeons & Dragons: The Official Home of D&D.” D&D Beyond, Wizards of the Coast, 8 Dec. 2025, www.dndbeyond.com/."
  },
  {
    "objectID": "Presentation.html#content-and-language-integrated-learning-and-cognition",
    "href": "Presentation.html#content-and-language-integrated-learning-and-cognition",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Content and Language Integrated-Learning and Cognition",
    "text": "Content and Language Integrated-Learning and Cognition\nA few permutation tests and a discussion on CLIL in psycholinguistics, as well as Language-Switching Costs and the Adaptive Control Model."
  },
  {
    "objectID": "Presentation.html#clil",
    "href": "Presentation.html#clil",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "CLIL",
    "text": "CLIL\n\nContent and Language Integrated-Learning\nstudents are taught non-language study subjects in a language that they are still learning\nintended increase efficiency: students will, in theory, learn the subject material while also improving in their target language\nthe cognitive burden of language-switching costs (LSC) may outweigh the possible benefits of CLIL"
  },
  {
    "objectID": "Presentation.html#language-switching-costs",
    "href": "Presentation.html#language-switching-costs",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Language Switching Costs",
    "text": "Language Switching Costs\n\nencoding-specificity hypothesis\nlanguage dependent knowledge representation framework\nboth claim that information that is learned in one language is ‘encoded’ in that language and the representation of knowledge is language-dependent\nessentially, that information learned in one language context is difficult to retrieve in other language contexts\njust one problem…"
  },
  {
    "objectID": "Presentation.html#dual-activation-model",
    "href": "Presentation.html#dual-activation-model",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Dual Activation Model",
    "text": "Dual Activation Model\n\nvery well-studied and documented phenomenon\nthe model of bilingual cognition the field currently operates under\nall languages that a bilingual person uses are activated simultaneously\nhow to get around this conflict?"
  },
  {
    "objectID": "Presentation.html#adaptive-control-hypothesis",
    "href": "Presentation.html#adaptive-control-hypothesis",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Adaptive Control Hypothesis",
    "text": "Adaptive Control Hypothesis\n\n\n\nproposed in Green & Abutalebi, 2013, expanded upon by Bialystok & Craik (2022)\nidentifies three types of bilingual language use contexts\neach context has different demand\n\n\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect."
  },
  {
    "objectID": "Presentation.html#adaptive-control-hypothesis-1",
    "href": "Presentation.html#adaptive-control-hypothesis-1",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Adaptive Control Hypothesis",
    "text": "Adaptive Control Hypothesis\n\n\n\nsingle language: each language used in a distinct context\ndual language: both languages are used in the same context but with different speakers\n\ndense code-switching: where both languages are used in the same context with other bilingual speakers\n\n\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect."
  },
  {
    "objectID": "Presentation.html#clil-1",
    "href": "Presentation.html#clil-1",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "CLIL",
    "text": "CLIL\nSo why is CLIL hard? - CLIL students are often in single or dual language contexts - constant demand on Goal Maintenance and Interference Control processes - do these processes impact knowledge encoding and retrieval?"
  },
  {
    "objectID": "Presentation.html#downsides-of-clil",
    "href": "Presentation.html#downsides-of-clil",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Downsides of CLIL",
    "text": "Downsides of CLIL\n\nCLIL is new-ish and popular for language competency\npeople love to talk about the benefits (Dalton-Puffer, 2008)\nbut CLIL students struggle compared to their peers in monolingual education\nshown to perform worse overall or need more time to display the same level of knowledge (e.g., Lo and Lo, 2014, Dallinger et al., 2016, Piesche et al., 2016)"
  },
  {
    "objectID": "Presentation.html#lsc-and-clil",
    "href": "Presentation.html#lsc-and-clil",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "LSC and CLIL",
    "text": "LSC and CLIL\n\nknowledge is harder to retrieve from a different language than it was aquired in\nlanguage-switching is directly detrimental to retrieval-based learning (Wußing et al., 2023)\nretrieval-based learning (practise tests) known to be more effective than restudy-based learning\nbad news for CLIL students if LSC impact the testing effect"
  },
  {
    "objectID": "Presentation.html#wußing-et-al.-2023",
    "href": "Presentation.html#wußing-et-al.-2023",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Wußing et al., 2023",
    "text": "Wußing et al., 2023\nTaught 117 German-English bilinguals 20 math concepts\n\nSchematic overview of the three between-subjects conditions regarding language-switching (Wußing et al., 2023)."
  },
  {
    "objectID": "Presentation.html#some-hypotheses",
    "href": "Presentation.html#some-hypotheses",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Some Hypotheses",
    "text": "Some Hypotheses\nNull Hypothesis: Language switching has no effect on learning performance\nNull Hypothesis: Language switching has no effect on restudy learning performance\nNull Hypothesis: Language switching has no effect on retrieval learning performance\nNull Hypothesis: The timing of a language switch has no impact on learning performance"
  },
  {
    "objectID": "Presentation.html#the-data",
    "href": "Presentation.html#the-data",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "The Data",
    "text": "The Data\nThe column names were originally all in German, I used my best judgement when translating, but know that I am not a native German speaker\n\n#renamed all of the German variable names. I couldn't get the Proband:innen one to rename, I think because of the colon, but that's participant ID \n\nTesLaS_DataSet &lt;- TesLaS_DataSet |&gt; \n  rename(\n    trial_number = Nummer,\n    condition = Bedingung,\n    variant = Variante,\n    cued_correct_images = Summe_Korrekt_Abbildungen,\n    cued_correct_statements = Summe_Korrekt_Aussagen,\n    cued_correct_total = Summe_Gesamt,\n    cued_correct_testing = Summe_Korrekt_Testing,\n    cued_correct_restudy = Summe_Korrekt_Restudy,\n    testing_advantage_cued_recall = Testing_Vorteil_CuedRecall,\n    transfer_correct_images = Tra_Summe_Korrekt_Abbildungen,\n    transfer_correct_statements = Tra_Summe_Korrekt_Aussagen,\n    transfer_correct_total = Tra_Summe_Gesamt,\n    transfer_correct_testing = Korrekt_Testing_Transfer,\n    transfer_correct_restudy = Korrekt_Restudy_Transfer,\n    testing_advantage_transfer = Testing_Vorteil_Transfer\n  ) |&gt;\n  mutate(  #renamed the conditions so it's easier to follow later \n  condition = case_when(\n    condition == 1 ~ \"monolingual\",\n    condition == 2 ~ \"switching for final tests\",\n    condition == 3 ~ \"switching for subsequent learning\" )) \n\n\n\nhead(TesLaS_DataSet) #show 6 rows\n\n# A tibble: 6 × 16\n  `Proband:innen` trial_number condition   variant cued_correct_images\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 1onki16                    1 monolingual       1                   8\n2 4ietz13                    2 monolingual       4                   2\n3 3NAER21                    3 monolingual       3                   1\n4 2nees17                    4 monolingual       2                   1\n5 1alki14                    5 monolingual       1                   4\n6 4aner12                    6 monolingual       4                   7\n# ℹ 11 more variables: cued_correct_statements &lt;dbl&gt;, cued_correct_total &lt;dbl&gt;,\n#   cued_correct_testing &lt;dbl&gt;, cued_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_cued_recall &lt;dbl&gt;, transfer_correct_images &lt;dbl&gt;,\n#   transfer_correct_statements &lt;dbl&gt;, transfer_correct_total &lt;dbl&gt;,\n#   transfer_correct_testing &lt;dbl&gt;, transfer_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_transfer &lt;dbl&gt;"
  },
  {
    "objectID": "Presentation.html#conditions",
    "href": "Presentation.html#conditions",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Conditions",
    "text": "Conditions\n\nTesLaS_means &lt;- TesLaS_DataSet |&gt;\n  group_by(condition) |&gt; \n  summarise(   #collapses it all into the grouped conditions \n    mean_cued_correct_testing = mean(cued_correct_testing, na.rm = TRUE),\n    mean_cued_correct_restudy = mean(cued_correct_restudy, na.rm = TRUE),\n    mean_transfer_correct_testing = mean(transfer_correct_testing, na.rm = TRUE),\n    mean_transfer_correct_restudy = mean(transfer_correct_restudy, na.rm = TRUE),\n    n = n() #number of participants per condition \n  )\n\n\nTesLaS_means #return the tibble, should be 3x6 \n\n# A tibble: 3 × 6\n  condition mean_cued_correct_te…¹ mean_cued_correct_re…² mean_transfer_correc…³\n  &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 monoling…                   4.53                   2.5                    7.08\n2 switchin…                   3.34                   2.32                   6.92\n3 switchin…                   2.28                   1.87                   5.82\n# ℹ abbreviated names: ¹​mean_cued_correct_testing, ²​mean_cued_correct_restudy,\n#   ³​mean_transfer_correct_testing\n# ℹ 2 more variables: mean_transfer_correct_restudy &lt;dbl&gt;, n &lt;int&gt;"
  },
  {
    "objectID": "Presentation.html#null-hypothesis-1-language-switching-has-no-effect-on-learning-performance",
    "href": "Presentation.html#null-hypothesis-1-language-switching-has-no-effect-on-learning-performance",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Hypothesis 1: Language switching has no effect on learning performance",
    "text": "Null Hypothesis 1: Language switching has no effect on learning performance\n\ncompared the monolingual condition to the condition which switched for subsequent learning\ngrouped together the cued final test and transfer final test, I’m not trying to look at what kind of final test participants perform better on\ngrouped together the retrieval learning participants and the restudy learning participants, just looking at overall learning performace\n\n\nset.seed(47)\n\nperm1_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}"
  },
  {
    "objectID": "Presentation.html#null-dist",
    "href": "Presentation.html#null-dist",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Dist",
    "text": "Null Dist"
  },
  {
    "objectID": "Presentation.html#p-value",
    "href": "Presentation.html#p-value",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "p-value",
    "text": "p-value\n\nset.seed(47)\n\n#two-sided p value\nperm1_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200"
  },
  {
    "objectID": "Presentation.html#null-hypothesis-2-language-switching-has-no-effect-on-restudy-learning-performance",
    "href": "Presentation.html#null-hypothesis-2-language-switching-has-no-effect-on-restudy-learning-performance",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Hypothesis 2: Language switching has no effect on restudy learning performance",
    "text": "Null Hypothesis 2: Language switching has no effect on restudy learning performance\n\ncompared the monolingual condition to the condition which switched for subsequent learning\ngrouped together the cued final test and transfer final test\nonly looked at the participants who used restudy learning\n\n\nset.seed(47)\n\nperm2_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_restudy, transfer_correct_restudy) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_restudy + transfer_correct_restudy) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}"
  },
  {
    "objectID": "Presentation.html#null-dist-1",
    "href": "Presentation.html#null-dist-1",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Dist",
    "text": "Null Dist"
  },
  {
    "objectID": "Presentation.html#p-value-1",
    "href": "Presentation.html#p-value-1",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "p-value",
    "text": "p-value\n\nset.seed(47)\n\n#two-sided p value\nperm2_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1     0.615"
  },
  {
    "objectID": "Presentation.html#null-hypothesis-3-language-switching-has-no-effect-on-retrieval-learning-performance",
    "href": "Presentation.html#null-hypothesis-3-language-switching-has-no-effect-on-retrieval-learning-performance",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Hypothesis 3: Language switching has no effect on retrieval learning performance",
    "text": "Null Hypothesis 3: Language switching has no effect on retrieval learning performance\n\ncompared the monolingual condition to the condition which switched for subsequent learning\ngrouped together the cued final test and transfer final test\nonly looked at the participants who used retrieval learning\n\n\nset.seed(47)\n\nperm3_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}"
  },
  {
    "objectID": "Presentation.html#null-dist-2",
    "href": "Presentation.html#null-dist-2",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Dist",
    "text": "Null Dist"
  },
  {
    "objectID": "Presentation.html#p-value-2",
    "href": "Presentation.html#p-value-2",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "p-value",
    "text": "p-value\n\n#two-sided p value\n\nset.seed(47)\n\n#two-sided p value\nperm3_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200"
  },
  {
    "objectID": "Presentation.html#null-hypothesis-4-the-timing-of-a-language-switch-has-no-impact-on-learning-performance",
    "href": "Presentation.html#null-hypothesis-4-the-timing-of-a-language-switch-has-no-impact-on-learning-performance",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Hypothesis 4: The timing of a language switch has no impact on learning performance",
    "text": "Null Hypothesis 4: The timing of a language switch has no impact on learning performance\n\ncompared the condition which switched for the final test to the condition which switched for subsequent learning\nwanted to see if when the language was switched would impact performance\ngrouped together the cued final test and transfer final test\ngrouped the retrieval learning participants and the restudy learning participants\n\n\nset.seed(47)\n\nperm4_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"switching for final tests\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}"
  },
  {
    "objectID": "Presentation.html#null-dist-3",
    "href": "Presentation.html#null-dist-3",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Null Dist",
    "text": "Null Dist"
  },
  {
    "objectID": "Presentation.html#p-value-3",
    "href": "Presentation.html#p-value-3",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "p-value",
    "text": "p-value\n\nset.seed(47)\n\n#two-sided p value\nperm4_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1    0.0220"
  },
  {
    "objectID": "Presentation.html#wußing-et-al.-2023-1",
    "href": "Presentation.html#wußing-et-al.-2023-1",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Wußing et al., 2023",
    "text": "Wußing et al., 2023\nReported Results:\n\nParticipants performed worse in conditions with language-switching\nLanguage-switching had a more significant detrimental effect on retrieval-based learning than on restudy-based learning\nLanguage switching had a more significant effect when the switch occurred after the initial learning phase and before subsequent learning, and in fact LSC only occurred with the switching for subsequent learning group"
  },
  {
    "objectID": "Presentation.html#wußing-et-al.-2023-2",
    "href": "Presentation.html#wußing-et-al.-2023-2",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "Wußing et al., 2023",
    "text": "Wußing et al., 2023\nPermuted Results:\n\nRejected Null Hypothesis: Language switching has no effect on learning performance\nCannot Reject Null Hypothesis: Language switching has no effect on restudy learning performance\nRejected Null Hypothesis: Language switching has no effect on retrieval learning performance\nRejected Null Hypothesis: The timing of a language switch has no impact on learning performance"
  },
  {
    "objectID": "Presentation.html#references",
    "href": "Presentation.html#references",
    "title": "Content and Language Integrated-Learning and Cognition",
    "section": "References",
    "text": "References\nAll data in this project cames from Wußing et al., 2023. The anonymous participant trial data is available for public access and use through the Center for Open Science (aka Open Science Framework, or OSF).\nGreen, D. W., & Abutalebi, J. (2013). Language control in bilinguals: The adaptive control hypothesis. Journal of Cognitive Psychology, 25(5), 515–530.https://doi.org/10.1080/20445911.2013.796377\nWußing, M., Grabner, R. H., Sommer, H., & Saalbach, H. (2023). Language-switching and retrieval-based learning: An unfavorable combination. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1198117"
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Content and Language Integrated-Learning",
    "section": "",
    "text": "If you are a Pomona student who has gone abroad, is looking into studying abroad, or - like me - backed out of studying abroad, you might know that many of the abroad programs Pomona offers in countries where English is not the dominant language for academia require you to take at least half, if not all, of your courses in your target language. This type of instruction is called Content and Language Integrated-Learning (CLIL). I personally ended up backing out of studying abroad after completing the entire process because, despite the program only requiring French 044 and merely recommending French 101, I did not feel ready to take 5 classes entirely in French at the end of my French 101 course. Despite passing with an A, and therefore supposedly knowing how to read and write at an academic level in my target language, I felt under-confident in my abilities. The previous semester, I had taken French Phonetics - a non-language course taught entirely in French - and while it had been fascinating and rewarding, it was also one of the hardest classes I had ever taken. So the ideal of CLIL was extremely daunting. But why is CLIL practiced? What are the benefits? Do students really struggle in CLIL programs, or did I just take a particularly brutal phonetics course?\nCLIL is a method of bilingual instruction that was made popular relatively recently, where students are taught non-language study subjects in a language that they are still learning. This is intended to work as a sort of efficiency increase; students will, in theory, learn the subject material while also improving in their target language.\nHowever, it is theorized that the cognitive burden of language-switching costs (LSC) outweigh the possible benefits of CLIL. The encoding-specificity hypothesis, or the language dependent knowledge representation framework, claim that information that is learned in one language is thus ‘encoded’ in that language, or that the representation of knowledge is language-dependent. Essentially, that information learned in one language context is difficult to retrieve in other language contexts. This is quite the claim, as the dual-activation model, which is the model of bilingual cognition the field currently operates under, states that all languages that a bilingual person uses are activated simultaneously.\nHowever, encoding-specificity might have nothing to do with dual-activation at all. While there has always been great debate as to whether bilingualism has an effect on cognitive control processes, a relatively new model of cognitive control (proposed in Green & Abutalebi, 2013) breaks down both cognitive control and bilingualism into more specific categories. This new model, the Adaptive Control Hypothesis, identifies three types of bilingual language use contexts: single language, where each language is used in a distinct context; dual language, where both languages are used in the same context but with different speakers who may speak only one of the bilingual’s languages; and dense code-switching, where both languages are used in the same context but with other bilingual speakers. Bialystok & Craik (2022) built upon the hypothesis, defining which of the 7 control processes received higher demand in each of the 3 interactional contexts of bilingual speakers.\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect.\n\n\nUnder the Adaptive Control Hypothesis, it could be theorized that because single-language context use is the context experienced by CLIL students, perhaps Goal Maintenance and Interference Control have the most impact on knowledge encoding and retrieval.\nThough many studies report benefits of CLIL when it comes to language competencies (Dalton-Puffer, 2008), many others report that students in CLIL struggle compared to students in monolingual education, either performing worse overall or needing more time to display the same level of knowledge as their monolingual peers (e.g., Lo and Lo, 2014, Dallinger et al., 2016, Piesche et al., 2016).\nRecent studies in particular focus on LSC, reporting worse performance or longer reaction times when knowledge is retrieved from a different language than it was encoded or acquired in. Wußing et al., 2023, claims that language-switching is directly detrimental to retrieval-based learning. Retrieval-based learning, which has been studied mostly through the “testing-effect”, is a type of learning where a practice test is taken after an initial learning phase to induce the test-taker to “retrieve” the information. Retrieval-based learning consistently increases learning performance compared to the restudy method (where participants restudy the same information after the initial learning phase). Because retrieval-based learning is so effective, it would be concerning for CLIL students if LSC is directly detrimental. Additionally, if different intervals of language-switching had different effects, this might indicate which bilingual use context has the greatest impact on retrieval-based learning. After reading the study, I wanted to use the lens of the Adaptive Control Model to look at the data that was collected.\nAll data in this project comes from Wußing et al., 2023, whose anonymous participant trial data is available for public access and use through the Center for Open Science (aka Open Science Framework, or OSF), a non-profit organization which aims to “increase the openness, integrity, and reproducibility of scientific research and scholarly communication” as an open-source database."
  },
  {
    "objectID": "Project3.html#clil-and-cognition",
    "href": "Project3.html#clil-and-cognition",
    "title": "Content and Language Integrated-Learning",
    "section": "",
    "text": "If you are a Pomona student who has gone abroad, is looking into studying abroad, or - like me - backed out of studying abroad, you might know that many of the abroad programs Pomona offers in countries where English is not the dominant language for academia require you to take at least half, if not all, of your courses in your target language. This type of instruction is called Content and Language Integrated-Learning (CLIL). I personally ended up backing out of studying abroad after completing the entire process because, despite the program only requiring French 044 and merely recommending French 101, I did not feel ready to take 5 classes entirely in French at the end of my French 101 course. Despite passing with an A, and therefore supposedly knowing how to read and write at an academic level in my target language, I felt under-confident in my abilities. The previous semester, I had taken French Phonetics - a non-language course taught entirely in French - and while it had been fascinating and rewarding, it was also one of the hardest classes I had ever taken. So the ideal of CLIL was extremely daunting. But why is CLIL practiced? What are the benefits? Do students really struggle in CLIL programs, or did I just take a particularly brutal phonetics course?\nCLIL is a method of bilingual instruction that was made popular relatively recently, where students are taught non-language study subjects in a language that they are still learning. This is intended to work as a sort of efficiency increase; students will, in theory, learn the subject material while also improving in their target language.\nHowever, it is theorized that the cognitive burden of language-switching costs (LSC) outweigh the possible benefits of CLIL. The encoding-specificity hypothesis, or the language dependent knowledge representation framework, claim that information that is learned in one language is thus ‘encoded’ in that language, or that the representation of knowledge is language-dependent. Essentially, that information learned in one language context is difficult to retrieve in other language contexts. This is quite the claim, as the dual-activation model, which is the model of bilingual cognition the field currently operates under, states that all languages that a bilingual person uses are activated simultaneously.\nHowever, encoding-specificity might have nothing to do with dual-activation at all. While there has always been great debate as to whether bilingualism has an effect on cognitive control processes, a relatively new model of cognitive control (proposed in Green & Abutalebi, 2013) breaks down both cognitive control and bilingualism into more specific categories. This new model, the Adaptive Control Hypothesis, identifies three types of bilingual language use contexts: single language, where each language is used in a distinct context; dual language, where both languages are used in the same context but with different speakers who may speak only one of the bilingual’s languages; and dense code-switching, where both languages are used in the same context but with other bilingual speakers. Bialystok & Craik (2022) built upon the hypothesis, defining which of the 7 control processes received higher demand in each of the 3 interactional contexts of bilingual speakers.\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect.\n\n\nUnder the Adaptive Control Hypothesis, it could be theorized that because single-language context use is the context experienced by CLIL students, perhaps Goal Maintenance and Interference Control have the most impact on knowledge encoding and retrieval.\nThough many studies report benefits of CLIL when it comes to language competencies (Dalton-Puffer, 2008), many others report that students in CLIL struggle compared to students in monolingual education, either performing worse overall or needing more time to display the same level of knowledge as their monolingual peers (e.g., Lo and Lo, 2014, Dallinger et al., 2016, Piesche et al., 2016).\nRecent studies in particular focus on LSC, reporting worse performance or longer reaction times when knowledge is retrieved from a different language than it was encoded or acquired in. Wußing et al., 2023, claims that language-switching is directly detrimental to retrieval-based learning. Retrieval-based learning, which has been studied mostly through the “testing-effect”, is a type of learning where a practice test is taken after an initial learning phase to induce the test-taker to “retrieve” the information. Retrieval-based learning consistently increases learning performance compared to the restudy method (where participants restudy the same information after the initial learning phase). Because retrieval-based learning is so effective, it would be concerning for CLIL students if LSC is directly detrimental. Additionally, if different intervals of language-switching had different effects, this might indicate which bilingual use context has the greatest impact on retrieval-based learning. After reading the study, I wanted to use the lens of the Adaptive Control Model to look at the data that was collected.\nAll data in this project comes from Wußing et al., 2023, whose anonymous participant trial data is available for public access and use through the Center for Open Science (aka Open Science Framework, or OSF), a non-profit organization which aims to “increase the openness, integrity, and reproducibility of scientific research and scholarly communication” as an open-source database."
  },
  {
    "objectID": "Project3.html#wußing-et-al.-2023",
    "href": "Project3.html#wußing-et-al.-2023",
    "title": "Content and Language Integrated-Learning",
    "section": "Wußing et al., 2023",
    "text": "Wußing et al., 2023\nThe study recruited 117 German-English bilingual participants, who were broken down into 6 groups. Half of the participants learned mathematical concepts with a practice-test, and half with a restudy opportunity, creating two within-subject conditions. These groups were then divided into thirds, with one third not switching languages, one switching only for the final test, and another switching between initial and subsequent learning, thus creating three between-subject conditions.\nOn the first day, all participants completed the initial learning phase. There were two texts, both with 10 paragraphs with each paragraph containing a single math concept. Each text was presented twice. After that, the restudy group was shown each concept individually to re-read, while the practice-test group filled in the blank concept name for each paragraph. This phase was also repeated twice.\nAfter 7 days, participants performed a cued recall test and a transfer test. In the cued recall test, participants entered the concept name that matched the presented description. In the transfer test, participants evaluated the correctness of a given statement.\n\n\n\nSchematic overview of the three between-subjects conditions regarding language-switching (Wußing et al., 2023).\n\n\nThe study claims that the results from these tests show the following main effects:\n\nParticipants performed better for the items learned via retrieval-based learning\nParticipants performed worse in conditions with language-switching\nLanguage-switching had a more significant detrimental effect on retrieval-based learning than on restudy-based learning\nLanguage switching had a more significant effect when the switch occurred after the initial learning phase and before subsequent learning, and in fact LSC only occurred with the switching for subsequent learning group\n\nThese claims give us a lot to explore. I have a few permutation tests I want to run on the data. Based on the claims the study makes, here are some Null Hypotheses that I am interested in. While I could use this data to see if retrieval-based learning correlates to better performance, retrieval-based learning is already very well studied and I am more interested in the psycholinguistics questions than the cogsci questions.\nNull Hypothesis: Language switching has no effect on learning performance\nNull Hypothesis: Language switching has no effect on restudy learning performance\nNull Hypothesis: Language switching has no effect on retrieval learning performance\nNull Hypothesis: The timing of a language switch has no impact on learning performance\nLet’s explore the raw data for a moment. The column names were originally all in German, so I used my best judgement when translating, but know that I am not a native German speaker.\n\n\nShow the code\n#renamed all of the German variable names. I couldn't get the Proband:innen one to rename, I think because of the colon, but that's participant ID \n\nTesLaS_DataSet &lt;- TesLaS_DataSet |&gt; \n  rename(\n    trial_number = Nummer,\n    condition = Bedingung,\n    variant = Variante,\n    cued_correct_images = Summe_Korrekt_Abbildungen,\n    cued_correct_statements = Summe_Korrekt_Aussagen,\n    cued_correct_total = Summe_Gesamt,\n    cued_correct_testing = Summe_Korrekt_Testing,\n    cued_correct_restudy = Summe_Korrekt_Restudy,\n    testing_advantage_cued_recall = Testing_Vorteil_CuedRecall,\n    transfer_correct_images = Tra_Summe_Korrekt_Abbildungen,\n    transfer_correct_statements = Tra_Summe_Korrekt_Aussagen,\n    transfer_correct_total = Tra_Summe_Gesamt,\n    transfer_correct_testing = Korrekt_Testing_Transfer,\n    transfer_correct_restudy = Korrekt_Restudy_Transfer,\n    testing_advantage_transfer = Testing_Vorteil_Transfer\n  ) |&gt;\n  mutate(  #renamed the conditions so it's easier to follow later \n  condition = case_when(\n    condition == 1 ~ \"monolingual\",\n    condition == 2 ~ \"switching for final tests\",\n    condition == 3 ~ \"switching for subsequent learning\" )) \n\n\n\nhead(TesLaS_DataSet) #show 6 rows\n\n\n# A tibble: 6 × 16\n  `Proband:innen` trial_number condition   variant cued_correct_images\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 1onki16                    1 monolingual       1                   8\n2 4ietz13                    2 monolingual       4                   2\n3 3NAER21                    3 monolingual       3                   1\n4 2nees17                    4 monolingual       2                   1\n5 1alki14                    5 monolingual       1                   4\n6 4aner12                    6 monolingual       4                   7\n# ℹ 11 more variables: cued_correct_statements &lt;dbl&gt;, cued_correct_total &lt;dbl&gt;,\n#   cued_correct_testing &lt;dbl&gt;, cued_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_cued_recall &lt;dbl&gt;, transfer_correct_images &lt;dbl&gt;,\n#   transfer_correct_statements &lt;dbl&gt;, transfer_correct_total &lt;dbl&gt;,\n#   transfer_correct_testing &lt;dbl&gt;, transfer_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_transfer &lt;dbl&gt;\n\n\nRecall that there are three conditions: not switching languages, switching only for the final test, and switching between initial and subsequent learning stages. I am most interested in these variables: cued_correct_testing, cued_correct_restudy, transfer_correct_testing, and transfer_correct_restudy. Let’s take a glimpse at the observed means of each of these variables, just to get a feel for the data.\n\n\nShow the code\nTesLaS_means &lt;- TesLaS_DataSet |&gt;\n  group_by(condition) |&gt; \n  summarise(   #collapses it all into the grouped conditions \n    mean_cued_correct_testing = mean(cued_correct_testing, na.rm = TRUE),\n    mean_cued_correct_restudy = mean(cued_correct_restudy, na.rm = TRUE),\n    mean_transfer_correct_testing = mean(transfer_correct_testing, na.rm = TRUE),\n    mean_transfer_correct_restudy = mean(transfer_correct_restudy, na.rm = TRUE),\n    n = n() #number of participants per condition \n  )\n\n\nTesLaS_means #return the tibble, should be 3x6 \n\n\n# A tibble: 3 × 6\n  condition mean_cued_correct_te…¹ mean_cued_correct_re…² mean_transfer_correc…³\n  &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 monoling…                   4.53                   2.5                    7.08\n2 switchin…                   3.34                   2.32                   6.92\n3 switchin…                   2.28                   1.87                   5.82\n# ℹ abbreviated names: ¹​mean_cued_correct_testing, ²​mean_cued_correct_restudy,\n#   ³​mean_transfer_correct_testing\n# ℹ 2 more variables: mean_transfer_correct_restudy &lt;dbl&gt;, n &lt;int&gt;\n\n\nImmediately we can see that for the monolingual condition, the average correct responses for the final tests are higher for the participants who did the practice tests, or who used retrieval learning.\nHowever, when it comes to the group that switched languages halfway through learning, their average correct responses are noticeably lower than the other two conditions when using retrieval learning.\nLet’s run some permutation tests, looking at each of the Null Hypotheses one at a time.\n\nNull Hypothesis 1: Language switching has no effect on learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning. I also grouped together the cued final test and transfer final test, which I will do in every permutation. This is because I am not trying to look at what kind of final test participants perform better on. For this first permutation, I also grouped together the retrieval learning participants and the restudy learning participants, because I am looking at overall learning performace.\n\n\nShow the code\nset.seed(47)\n\nperm1_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm1_stats &lt;- \n  map(1:500, perm1_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm1_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm1_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200\n\n\n\n\nNull Hypothesis 2: Language switching has no effect on restudy learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning and grouped together the cued final test and transfer final test, as in permutation 1. For this permutation, I only looked at the participants who used restudy learning.\n\n\nShow the code\nset.seed(47)\n\nperm2_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_restudy, transfer_correct_restudy) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_restudy + transfer_correct_restudy) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm2_stats &lt;- \n  map(1:500, perm2_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm2_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm2_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1     0.615\n\n\n\n\nNull Hypothesis 3: Language switching has no effect on retrieval learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning and grouped together the cued final test and transfer final test, as in permutation 1. For this permutation, I only looked at the participants who used retrieval learning.\n\n\nShow the code\nset.seed(47)\n\nperm3_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm3_stats &lt;- \n  map(1:500, perm3_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm3_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n#two-sided p value\n\nset.seed(47)\n\n#two-sided p value\nperm3_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200\n\n\n\n\nNull Hypothesis 4: The timing of a language switch has no impact on learning performance\nFor this permutation, I compared the condition which switched for the final test to the condition which switched for subsequent learning. This is because I wanted to see if when the language was switched would impact performance. I grouped together the cued final test and transfer final test, and the retrieval learning participants and the restudy learning participants.\n\n\nShow the code\nset.seed(47)\n\nperm4_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"switching for final tests\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm4_stats &lt;- \n  map(1:500, perm4_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm4_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm4_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1    0.0220"
  },
  {
    "objectID": "Project3.html#permuted-results",
    "href": "Project3.html#permuted-results",
    "title": "Content and Language Integrated-Learning",
    "section": "Permuted Results:",
    "text": "Permuted Results:\nRejected Null Hypothesis: Language switching has no effect on learning performance\nCannot Reject Null Hypothesis: Language switching has no effect on restudy learning performance\nRejected Null Hypothesis: Language switching has no effect on retrieval learning performance\nRejected Null Hypothesis: The timing of a language switch has no impact on learning performance\nKnowing that LSC affect learning performance, particularly retrieval and particularly when the language is changed between subjects, is very useful knowledge for CLIL students. Taking all of your courses in a language you aren’t fluent in is hard enough - knowing what will make it harder is important. I know that as I move forward with French courses, I will keep in mind that I’ll only be making things more difficult for myself if I try to mix English into my studies.\nIt’s also comforting to me, and to anyone else who felt ashamed for backing out of CLIL or who is struggling with CLIL now, to know that the demands that language-switching places on your cognition are very real and capable of disrupting the most tried and true study methods."
  },
  {
    "objectID": "Project3.html#references",
    "href": "Project3.html#references",
    "title": "Content and Language Integrated-Learning",
    "section": "References",
    "text": "References\nGreen, D. W., & Abutalebi, J. (2013). Language control in bilinguals: The adaptive control hypothesis. Journal of Cognitive Psychology, 25(5), 515–530.https://doi.org/10.1080/20445911.2013.796377\nWußing, M., Grabner, R. H., Sommer, H., & Saalbach, H. (2023). Language-switching and retrieval-based learning: An unfavorable combination. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1198117"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Rowan Norenberg is a linguistics, data science, and French student. When they’re not in class or studying, they perform in choir and run D&D campaigns for their friends.\n\n\nPomona College | Claremont, CA B.A. in Linguistics | Sept 2023 - June 2027"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Pomona College | Claremont, CA B.A. in Linguistics | Sept 2023 - June 2027"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sophie Rowan Norenberg",
    "section": "",
    "text": "Rowan is a student of linguistics, data science, and French at Pomona College. When they’re not in class or studying, Rowan performs in multiple choirs and acts as Vice President for an on-campus club with over 100 members.\n\n\nPomona College | Claremont, CA | B.A. in Linguistics | Minor in French | Minor in Data Science | Sept 2023 - Expected May 2027\n\n\n\nLinguistics Course Work:\n\nIntroduction to Linguistics\nPhonology\nSyntactic Analysis\nSemantics & Pragmatics\nLanguage Acquisition\nWriting Systems\nPsycholinguistics\n\n\n\n\n\nFrench American School of Puget Sound | Classroom Assistant | May 2024 - August 2024\nPomona College Choir | Choir Manager | August 2025 - Present\n\n\n\n\nLaTeX\n\nR"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sophie Rowan Norenberg",
    "section": "",
    "text": "Pomona College | Claremont, CA | B.A. in Linguistics | Minor in French | Minor in Data Science | Sept 2023 - Expected May 2027"
  },
  {
    "objectID": "index.html#academic-experience",
    "href": "index.html#academic-experience",
    "title": "Sophie Rowan Norenberg",
    "section": "",
    "text": "Linguistics Course Work:\n\nIntroduction to Linguistics\nPhonology\nSyntactic Analysis\nSemantics & Pragmatics\nLanguage Acquisition\nWriting Systems\nPsycholinguistics"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Sophie Rowan Norenberg",
    "section": "",
    "text": "French American School of Puget Sound | Classroom Assistant | May 2024 - August 2024\nPomona College Choir | Choir Manager | August 2025 - Present"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Sophie Rowan Norenberg",
    "section": "",
    "text": "LaTeX\n\nR"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Project 5",
    "section": "",
    "text": "As someone who lives in Seattle, Washington, and has gone my whole like making jokes about out-of-towners being terrible Seattle drivers, I had to know: which State gets pulled over and arrested the most when driving in Seattle? (besides native Washingtonians, of course.)\n\nSELECT\nvehicle_registration_state,\nCOUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state IS NOT NULL\nGROUP BY vehicle_registration_state\nORDER BY num_arrests DESC\nLIMIT 0, 10;\n\n\nDisplaying records 1 - 10\n\n\nvehicle_registration_state\nnum_arrests\n\n\n\n\nWA\n11219\n\n\nHI\n2692\n\n\nIL\n2019\n\n\nOR\n997\n\n\nND\n783\n\n\nMA\n689\n\n\nCA\n534\n\n\nME\n223\n\n\nIN\n182\n\n\nCO\n154\n\n\n\n\n\nSome of these trends are unsurprising - of course, people who live in Washington and therefore have Washington liscence plates will have the most traffic stops. Nearby states like Oregon, North Dakota, Colorado, and California (all of which have high migration rates to Seattle) are also unsurprising. But cars registered in Hawaii and Illinois have the highest rate of traffic stops in Seattle besides Washington. Why? Neither are particularly close to Washington, nor do they have high rates of migration. I guessed that the answer would lie in rentals. Rental car companies famously register and insure their cars wherever will give them the cheapest rates. Hawaii has the lowest average yearly car registration fee in the US, at a mere $12. Illinois has one of the highest, though, at over $150. However, another look at the data reveals that perhaps Seattle PD just doesn’t do a very good job of documenting what state the plates are from, for some reason.\n\nSELECT 'WA' AS registration_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state = 'WA'\nUNION ALL\nSELECT 'Out-of-State' AS reg_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state &lt;&gt; 'WA'\n  AND vehicle_registration_state IS NOT NULL  \nUNION ALL\nSELECT 'Unknown' AS reg_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state IS NULL;\n\n\n3 records\n\n\nregistration_state\nnum_arrests\n\n\n\n\nWA\n11219\n\n\nOut-of-State\n9011\n\n\nUnknown\n299729\n\n\n\n\n\n299,729 unknown values. In conclusion, the Seattle PD is really bad at recording the states of plates during traffic stops. I had hoped that the Seattle PD would be better about recording plate state when an actual arrest was made, but…\n\nSELECT 'WA' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state = 'WA'\nUNION ALL\n\nSELECT 'Out-of-State' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state &lt;&gt; 'WA'\n  AND vehicle_registration_state IS NOT NULL\nUNION ALL\n\nSELECT 'Unknown' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state IS NULL;\n\n\n3 records\n\n\nregistration_state\nnum_arrests\n\n\n\n\nWA\n938\n\n\nOut-of-State\n620\n\n\nUnknown\n8999\n\n\n\n\n\nNot really.\nHowever: Seattle is a bit unique about plates, in that in 2009, they switched to a 7-character plate system (CCC-####) and at first, EVERY plate started with an A. Then around 2015, when they ran out of unique combinations, a B. By sometime after 2020, newly issued plates started with a C.\nNot so helpful when trying to identify a criminal, because if you only remember the first letter of a plate, it’s no help at all. But, it does mean that ALL Washington licence plates start with an A, B, or C (since 2009) (unless they are customized). This probably means that Seattle PD is only recording “WA” when they feel like it’s necessary; eg, when the plate number is really old or customized. This would help explain the sheer number of NULL values. Any plate that starts with an A, B, or C and fits the CCC-#### system is going to be a Washington plate, so there’s no need to write that down, and any OTHER plate that isn’t customized and marked as WA is an out-of-towner.\nSo what does this have to do with Illinois? Well, in 2017, Illinois ALSO implemented a 7-character plate system (CC-#####) which started with A. So when a car with an Illinois plate is pulled over, the Seattle PD officer can’t just record the plate and leave the state blank, because that would imply that the state was Washington. So, they specifically record “IL”.\nUnfortunately, both the statewide Illinois data table and the Chicago data table do not include a row for vehicle registration state, so I can’t compare this trend to Illinois.\nNext I was exploring which police departments had more possible entries for subject race than others. North Dakota, for example, had the standard lineup (White, Asian, African American, Hispanic, Other) but with the distinction of one group that is lumped into “Other” in many states: Native American. I decided to look at the ages of traffic stops made on Native Americans, as I had noticed that many violations were for driving without a licence.\n\nSELECT AVG(subject_age) AS avg_age_all\nFROM nd_statewide_2020_04_01;\n\n\n1 records\n\n\navg_age_all\n\n\n\n\n36.0913\n\n\n\n\n\n\nSELECT AVG(subject_age) AS avg_age_native\nFROM nd_statewide_2020_04_01\nWHERE raw_Race = 'Native American';\n\n\n1 records\n\n\navg_age_native\n\n\n\n\n33.6656\n\n\n\n\n\n\nSELECT\n    100.0 * \n        (SELECT COUNT(*) \n         FROM nd_statewide_2020_04_01 \n         WHERE raw_Race = 'Native American' AND subject_age &lt; 18)\n    /\n        (SELECT COUNT(*)\n         FROM nd_statewide_2020_04_01\n         WHERE raw_Race = 'Native American')\n    AS pct_minor_native,\n\n    100.0 *\n        (SELECT COUNT(*) \n         FROM nd_statewide_2020_04_01 \n         WHERE raw_Race != 'Native American' AND subject_age &lt; 18)\n    /\n        (SELECT COUNT(*)\n         FROM nd_statewide_2020_04_01\n         WHERE raw_Race != 'Native American')\n    AS pct_minor_non_native;\n\n\n1 records\n\n\npct_minor_native\npct_minor_non_native\n\n\n\n\n3.6813\n3.11248\n\n\n\n\n\nNot only is the average age of Native Americans who get stopped in North Dakota lower than that of the total average, but there is a notable increase in Native Americans minors who get stopped. Most other data tables, including ones for areas with very high Native populations, do not list Native American as an option (I assume they are grouped under ‘other’.) Upon looking at Oklahoma, which is known for a high Native population, I found the abbreviated race ‘I’, which I believe stands for Indian, in this case referring to Native Americans (many official state and tribal documents in Oklahoma still use this language).\nHowever, Tulsa, Oklahoma also has some non-standard race abbreviations. I was intending to calculate the percent of arrests made on Native American minors compared to other races in Tulsa, as well, but I hit a bit of a bump.\n\nSELECT DISTINCT raw_race\nFROM ok_tulsa_2020_04_01;\n\n\nDisplaying records 1 - 10\n\n\nraw_race\n\n\n\n\nNA\n\n\nW\n\n\nB\n\n\nU\n\n\nH\n\n\nI\n\n\nO\n\n\nA\n\n\nD\n\n\nF\n\n\n\n\n\nSome of these are easy to find the code to, and are standard in the United States. W - White B - Black / African American U - Unknown H - Hispanic I - Native American / Native Alaskan A - Asian O - Other\nAfter that, though, it gets mushy. At first I thought they might be distinguishing individual tribal nations in the state, but given the sheer number of distinct tribes in Oklahoma (and that many of them start with the same letter) this seems unlikely, or at least unintuitive. I scoured the Tulsa PD official website and records for a key, but found nothing. I am still curious about these abbreviations and would love to be more informed on what these all might represent.\n\ndbDisconnect(con_traffic, shutdown = TRUE)\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "openpolicing.html",
    "href": "openpolicing.html",
    "title": "Project 5",
    "section": "",
    "text": "As someone who lives in Seattle, Washington, and has gone my whole like making jokes about out-of-towners being terrible Seattle drivers, I had to know: which State gets pulled over and arrested the most when driving in Seattle? (besides native Washingtonians, of course.)\n\n\nShow the code\nSELECT\nvehicle_registration_state,\nCOUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state IS NOT NULL\nGROUP BY vehicle_registration_state\nORDER BY num_arrests DESC\nLIMIT 0, 10;\n\n\n\nDisplaying records 1 - 10\n\n\nvehicle_registration_state\nnum_arrests\n\n\n\n\nWA\n11219\n\n\nHI\n2692\n\n\nIL\n2019\n\n\nOR\n997\n\n\nND\n783\n\n\nMA\n689\n\n\nCA\n534\n\n\nME\n223\n\n\nIN\n182\n\n\nCO\n154\n\n\n\n\n\nSome of these trends are unsurprising - of course, people who live in Washington and therefore have Washington liscence plates will have the most traffic stops. Nearby states like Oregon, North Dakota, Colorado, and California (all of which have high migration rates to Seattle) are also unsurprising. But cars registered in Hawaii and Illinois have the highest rate of traffic stops in Seattle besides Washington. Why? Neither are particularly close to Washington, nor do they have high rates of migration. I guessed that the answer would lie in rentals. Rental car companies famously register and insure their cars wherever will give them the cheapest rates. Hawaii has the lowest average yearly car registration fee in the US, at a mere $12. Illinois has one of the highest, though, at over $150. However, another look at the data reveals that perhaps Seattle PD just doesn’t do a very good job of documenting what state the plates are from, for some reason.\n\n\nShow the code\nSELECT 'WA' AS registration_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state = 'WA'\nUNION ALL\nSELECT 'Out-of-State' AS reg_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state &lt;&gt; 'WA'\n  AND vehicle_registration_state IS NOT NULL  \nUNION ALL\nSELECT 'Unknown' AS reg_state, \n  COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE vehicle_registration_state IS NULL;\n\n\n\n3 records\n\n\nregistration_state\nnum_arrests\n\n\n\n\nWA\n11219\n\n\nOut-of-State\n9011\n\n\nUnknown\n299729\n\n\n\n\n\n299,729 unknown values. In conclusion, the Seattle PD is really bad at recording the states of plates during traffic stops. I had hoped that the Seattle PD would be better about recording plate state when an actual arrest was made, but…\n\n\nShow the code\nSELECT 'WA' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state = 'WA'\nUNION ALL\n\nSELECT 'Out-of-State' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state &lt;&gt; 'WA'\n  AND vehicle_registration_state IS NOT NULL\nUNION ALL\n\nSELECT 'Unknown' AS registration_state, COUNT(*) AS num_arrests\nFROM wa_seattle_2020_04_01\nWHERE arrest_made = '1'\n  AND vehicle_registration_state IS NULL;\n\n\n\n3 records\n\n\nregistration_state\nnum_arrests\n\n\n\n\nWA\n938\n\n\nOut-of-State\n620\n\n\nUnknown\n8999\n\n\n\n\n\nNot really.\nHowever: Seattle is a bit unique about plates, in that in 2009, they switched to a 7-character plate system (CCC-####) and at first, EVERY plate started with an A. Then around 2015, when they ran out of unique combinations, a B. By sometime after 2020, newly issued plates started with a C.\nNot so helpful when trying to identify a criminal, because if you only remember the first letter of a plate, it’s no help at all. But, it does mean that ALL Washington licence plates start with an A, B, or C (since 2009) (unless they are customized). This probably means that Seattle PD is only recording “WA” when they feel like it’s necessary; eg, when the plate number is really old or customized. This would help explain the sheer number of NULL values. Any plate that starts with an A, B, or C and fits the CCC-#### system is going to be a Washington plate, so there’s no need to write that down, and any OTHER plate that isn’t customized and marked as WA is an out-of-towner.\nSo what does this have to do with Illinois? Well, in 2017, Illinois ALSO implemented a 7-character plate system (CC-#####) which started with A. So when a car with an Illinois plate is pulled over, the Seattle PD officer can’t just record the plate and leave the state blank, because that would imply that the state was Washington. So, they specifically record “IL”.\nUnfortunately, both the statewide Illinois data table and the Chicago data table do not include a row for vehicle registration state, so I can’t compare this trend to Illinois.\nNext I was exploring which police departments had more possible entries for subject race than others. North Dakota, for example, had the standard lineup (White, Asian, African American, Hispanic, Other) but with the distinction of one group that is lumped into “Other” in many states: Native American. I decided to look at the ages of traffic stops made on Native Americans, as I had noticed that many violations were for driving without a licence.\n\n\nShow the code\nSELECT AVG(subject_age) AS avg_age_all\nFROM nd_statewide_2020_04_01;\n\n\n\n1 records\n\n\navg_age_all\n\n\n\n\n36.0913\n\n\n\n\n\n\n\nShow the code\nSELECT AVG(subject_age) AS avg_age_native\nFROM nd_statewide_2020_04_01\nWHERE raw_Race = 'Native American';\n\n\n\n1 records\n\n\navg_age_native\n\n\n\n\n33.6656\n\n\n\n\n\n\n\nShow the code\nSELECT\n    100.0 * \n        (SELECT COUNT(*) \n         FROM nd_statewide_2020_04_01 \n         WHERE raw_Race = 'Native American' AND subject_age &lt; 18)\n    /\n        (SELECT COUNT(*)\n         FROM nd_statewide_2020_04_01\n         WHERE raw_Race = 'Native American')\n    AS pct_minor_native,\n\n    100.0 *\n        (SELECT COUNT(*) \n         FROM nd_statewide_2020_04_01 \n         WHERE raw_Race != 'Native American' AND subject_age &lt; 18)\n    /\n        (SELECT COUNT(*)\n         FROM nd_statewide_2020_04_01\n         WHERE raw_Race != 'Native American')\n    AS pct_minor_non_native;\n\n\n\n1 records\n\n\npct_minor_native\npct_minor_non_native\n\n\n\n\n3.6813\n3.11248\n\n\n\n\n\nNot only is the average age of Native Americans who get stopped in North Dakota lower than that of the total average, but there is a notable increase in Native Americans minors who get stopped. Most other data tables, including ones for areas with very high Native populations, do not list Native American as an option (I assume they are grouped under ‘other’.) Upon looking at Oklahoma, which is known for a high Native population, I found the abbreviated race ‘I’, which I believe stands for Indian, in this case referring to Native Americans (many official state and tribal documents in Oklahoma still use this language).\nHowever, Tulsa, Oklahoma also has some non-standard race abbreviations. I was intending to calculate the percent of arrests made on Native American minors compared to other races in Tulsa, as well, but I hit a bit of a bump.\n\n\nShow the code\nSELECT DISTINCT raw_race\nFROM ok_tulsa_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nraw_race\n\n\n\n\nNA\n\n\nW\n\n\nB\n\n\nU\n\n\nH\n\n\nI\n\n\nO\n\n\nA\n\n\nD\n\n\nF\n\n\n\n\n\nSome of these are easy to find the code to, and are standard in the United States. W - White B - Black / African American U - Unknown H - Hispanic I - Native American / Native Alaskan A - Asian O - Other\nAfter that, though, it gets mushy. At first I thought they might be distinguishing individual tribal nations in the state, but given the sheer number of distinct tribes in Oklahoma (and that many of them start with the same letter) this seems unlikely, or at least unintuitive. I scoured the Tulsa PD official website and records for a key, but found nothing. I am still curious about these abbreviations and would love to be more informed on what these all might represent.\n\n\nShow the code\ndbDisconnect(con_traffic, shutdown = TRUE)\n\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "TidyTuesdayShakespeare.html",
    "href": "TidyTuesdayShakespeare.html",
    "title": "Tidy Tuesday - Shakespeare",
    "section": "",
    "text": "The data from this page comes from Shakespeare’s works, which are in the public domain. The datasets used here pulled their data specifically from https://shakespeare.mit.edu/ and can be found in the TidyTeusday 2024 repository, curated by Nicola Rennie.\n\n\nShow the code\n#| echo: false\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(forcats)\n\n\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/romeo_juliet.csv')\n\n\n\n\nShow the code\ntragedies &lt;- bind_rows(\n  hamlet |&gt; mutate(play = \"Hamlet\"),\n  macbeth |&gt; mutate(play = \"Macbeth\"),\n  romeo_juliet |&gt; mutate(play = \"Romeo and Juliet\")\n)\n\n\ntragedies_ghosts &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"\\\\b(ghost|witch|spirit)\\\\b\")) |&gt;\n  select(play, dialogue) |&gt;\n  mutate(supernatural = str_extract(str_to_lower(dialogue), \"\\\\b(ghost|witch|spirit)\\\\b\" )) |&gt;\n  count(play, supernatural, name = \"num_mentions\")\n\n\n\nggplot(tragedies_ghosts, aes(x = play, y = num_mentions, fill = supernatural)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Mentions of 'Ghost', 'Witch', and 'Spirit' in Shakespeare's Tragedies\",\n    x = \"Play\",\n    y = \"Number of Mentions\",\n    fill = \"\" \n    ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThis graph shows that Hamlet has a great deal many more mentions of ghosts, witches, and spirits than the other two plays. This indicates that ghosts, witches, and spirits are more relevant to the plot of Hamlet than to the other two, which makes sense because Hamlet is frequently seeing apparitions of his dead father and this is central to the plot.\n\n\nShow the code\ntragedies_exclaim &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)\\\\bHamlet!+|Macbeth!+|Romeo!+|Juliet!+\")) |&gt;\n  mutate(exclamations = str_extract(dialogue, \"(?i)\\\\bHamlet!+|Macbeth!+|Romeo!+|Juliet!+\")) |&gt;\n  count(play, exclamations, name = \"num_exclaim\") |&gt;\n  mutate(exclamations = fct_relevel(exclamations, c(\"Hamlet!\", \"Macbeth!\", \"Romeo!\", \"Juliet!\")))\n\n\n\nggplot(tragedies_exclaim, aes(x = exclamations, y = num_exclaim, fill = exclamations)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Number of times the main character's name is exclaimed\",\n    x = \"Play\",\n    y = \"Number of Mentions\",\n    fill = \"\" \n    ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nThis graph shows that out of the main characters from each play, Romeo is the one who gets yelled at (or about) the most.\n\n\nShow the code\ntragedies_exclaimers &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)\\\\b(Hamlet|Macbeth|Romeo|Juliet)!+\")) |&gt; \n  mutate(\n    exclamations = str_extract(dialogue, \"(?i)\\\\b(Hamlet|Macbeth|Romeo|Juliet)!+\"),\n    exclamations = str_to_title(exclamations)  \n  ) |&gt;\n  count(play, character, exclamations, name = \"num_exclaim\") |&gt;\n  arrange(desc(num_exclaim))\n\n\ntop_exclaimers &lt;- tragedies_exclaimers |&gt; \n  slice_max(num_exclaim, n = 10)\n\nggplot(top_exclaimers, aes(\n  x = reorder(character, num_exclaim),\n  y = num_exclaim,\n  fill = exclamations\n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Characters Who Shout Names the Most\",\n    subtitle = \"Colored by which name they exclaim\",\n    x = \"Character\",\n    y = \"Number of Exclamations\",\n    fill = \"Name Shouted\"\n  ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nAnd this graph shows that not all of those times Romeo is yelled at are from Juliet’s famous line! Turns out Romeo is yelled at (or about) by a good variety of folks.\n\n\nShow the code\ntragedies_exclaimers &lt;- tragedies |&gt;\n  filter(str_detect(dialogue, \"(?i)!+\")) |&gt; \n  mutate(\n    exclamations = str_extract(dialogue, \"(?i)!+\"),\n    exclamations = str_to_title(exclamations)  \n  ) |&gt;\n  count(play, character, exclamations, name = \"num_exclaim\") |&gt;\n  arrange(desc(num_exclaim))\n\n# Optionally, focus on the top 10 loudest shouters\ntop_exclaimers &lt;- tragedies_exclaimers |&gt; \n  slice_max(num_exclaim, n = 10)\n\nggplot(top_exclaimers, aes(\n  x = reorder(character, num_exclaim),\n  y = num_exclaim,\n  fill = play\n)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Criers\",\n    subtitle = \"Characters in Shakespeare's Tragedies who Exclaim the Most\",\n    x = \"Character\",\n    y = \"Number of Exclamations\",\n    fill = \"\"\n  ) +\n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nFinally, this graph tells us that despite not being yelled at nearly as much as Romeo is, Hamlet yells a lot. Perhaps this is because of all of those ghosts, spirits, and witches.\nBelow is a graph for each play showing who has the most lines. These line counts graphs tell you which character is the “main character”, which, unsurprisingly, are also the title characters. But just in case.\n\n\nShow the code\nhamlet_counts &lt;- hamlet |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Hamlet\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(hamlet_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Hamlet: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmacbeth_counts &lt;- macbeth |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Macbeth\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(macbeth_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"goldenrod\") +\n  coord_flip() +\n  labs(\n    title = \"Macbeth: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nromeo_juliet_counts &lt;- romeo_juliet |&gt;\n  filter(!grepl(\"stage\", character, ignore.case = TRUE)) |&gt; \n  count(character, name = \"num_lines\") |&gt;\n  mutate(play = \"Romeo and Juliet\") |&gt;\n  arrange(desc(num_lines)) |&gt;\n  slice_head(n = 20)\n\n  \n  \nggplot(romeo_juliet_counts, aes(x = reorder(character, num_lines), y = num_lines)) +\n  geom_col(fill = \"maroon\") +\n  coord_flip() +\n  labs(\n    title = \"Romeo and Juliet: Number of Lines\",\n    x = \"Named Character\",\n    y = \"Total Number of Lines\"\n  ) +\n  theme_minimal(base_size = 14) +  # bigger text\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  ) + \n  theme_minimal() + \n  theme(plot.title = element_text(size = 16, face = \"bold\"))"
  },
  {
    "objectID": "CLILperm.html",
    "href": "CLILperm.html",
    "title": "Content and Language Integrated-Learning",
    "section": "",
    "text": "If you are a Pomona student who has gone abroad, is looking into studying abroad, or - like me - backed out of studying abroad, you might know that many of the abroad programs Pomona offers in countries where English is not the dominant language for academia require you to take at least half, if not all, of your courses in your target language. This type of instruction is called Content and Language Integrated-Learning (CLIL). I personally ended up backing out of studying abroad after completing the entire process because, despite the program only requiring French 044 and merely recommending French 101, I did not feel ready to take 5 classes entirely in French at the end of my French 101 course. Despite passing with an A, and therefore supposedly knowing how to read and write at an academic level in my target language, I felt under-confident in my abilities. The previous semester, I had taken French Phonetics - a non-language course taught entirely in French - and while it had been fascinating and rewarding, it was also one of the hardest classes I had ever taken. So the ideal of CLIL was extremely daunting. But why is CLIL practiced? What are the benefits? Do students really struggle in CLIL programs, or did I just take a particularly brutal phonetics course?\nCLIL is a method of bilingual instruction that was made popular relatively recently, where students are taught non-language study subjects in a language that they are still learning. This is intended to work as a sort of efficiency increase; students will, in theory, learn the subject material while also improving in their target language.\nHowever, it is theorized that the cognitive burden of language-switching costs (LSC) outweigh the possible benefits of CLIL. The encoding-specificity hypothesis, or the language dependent knowledge representation framework, claim that information that is learned in one language is thus ‘encoded’ in that language, or that the representation of knowledge is language-dependent. Essentially, that information learned in one language context is difficult to retrieve in other language contexts. This is quite the claim, as the dual-activation model, which is the model of bilingual cognition the field currently operates under, states that all languages that a bilingual person uses are activated simultaneously.\nHowever, encoding-specificity might have nothing to do with dual-activation at all. While there has always been great debate as to whether bilingualism has an effect on cognitive control processes, a relatively new model of cognitive control (proposed in Green & Abutalebi, 2013) breaks down both cognitive control and bilingualism into more specific categories. This new model, the Adaptive Control Hypothesis, identifies three types of bilingual language use contexts: single language, where each language is used in a distinct context; dual language, where both languages are used in the same context but with different speakers who may speak only one of the bilingual’s languages; and dense code-switching, where both languages are used in the same context but with other bilingual speakers. Bialystok & Craik (2022) built upon the hypothesis, defining which of the 7 control processes received higher demand in each of the 3 interactional contexts of bilingual speakers.\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect.\n\n\nUnder the Adaptive Control Hypothesis, it could be theorized that because single-language context use is the context experienced by CLIL students, perhaps Goal Maintenance and Interference Control have the most impact on knowledge encoding and retrieval.\nThough many studies report benefits of CLIL when it comes to language competencies (Dalton-Puffer, 2008), many others report that students in CLIL struggle compared to students in monolingual education, either performing worse overall or needing more time to display the same level of knowledge as their monolingual peers (e.g., Lo and Lo, 2014, Dallinger et al., 2016, Piesche et al., 2016).\nRecent studies in particular focus on LSC, reporting worse performance or longer reaction times when knowledge is retrieved from a different language than it was encoded or acquired in. Wußing et al., 2023, claims that language-switching is directly detrimental to retrieval-based learning. Retrieval-based learning, which has been studied mostly through the “testing-effect”, is a type of learning where a practice test is taken after an initial learning phase to induce the test-taker to “retrieve” the information. Retrieval-based learning consistently increases learning performance compared to the restudy method (where participants restudy the same information after the initial learning phase). Because retrieval-based learning is so effective, it would be concerning for CLIL students if LSC is directly detrimental. Additionally, if different intervals of language-switching had different effects, this might indicate which bilingual use context has the greatest impact on retrieval-based learning. After reading the study, I wanted to use the lens of the Adaptive Control Model to look at the data that was collected.\nAll data in this project comes from Wußing et al., 2023, whose anonymous participant trial data is available for public access and use through the Center for Open Science (aka Open Science Framework, or OSF), a non-profit organization which aims to “increase the openness, integrity, and reproducibility of scientific research and scholarly communication” as an open-source database."
  },
  {
    "objectID": "CLILperm.html#clil-and-cognition",
    "href": "CLILperm.html#clil-and-cognition",
    "title": "Content and Language Integrated-Learning",
    "section": "",
    "text": "If you are a Pomona student who has gone abroad, is looking into studying abroad, or - like me - backed out of studying abroad, you might know that many of the abroad programs Pomona offers in countries where English is not the dominant language for academia require you to take at least half, if not all, of your courses in your target language. This type of instruction is called Content and Language Integrated-Learning (CLIL). I personally ended up backing out of studying abroad after completing the entire process because, despite the program only requiring French 044 and merely recommending French 101, I did not feel ready to take 5 classes entirely in French at the end of my French 101 course. Despite passing with an A, and therefore supposedly knowing how to read and write at an academic level in my target language, I felt under-confident in my abilities. The previous semester, I had taken French Phonetics - a non-language course taught entirely in French - and while it had been fascinating and rewarding, it was also one of the hardest classes I had ever taken. So the ideal of CLIL was extremely daunting. But why is CLIL practiced? What are the benefits? Do students really struggle in CLIL programs, or did I just take a particularly brutal phonetics course?\nCLIL is a method of bilingual instruction that was made popular relatively recently, where students are taught non-language study subjects in a language that they are still learning. This is intended to work as a sort of efficiency increase; students will, in theory, learn the subject material while also improving in their target language.\nHowever, it is theorized that the cognitive burden of language-switching costs (LSC) outweigh the possible benefits of CLIL. The encoding-specificity hypothesis, or the language dependent knowledge representation framework, claim that information that is learned in one language is thus ‘encoded’ in that language, or that the representation of knowledge is language-dependent. Essentially, that information learned in one language context is difficult to retrieve in other language contexts. This is quite the claim, as the dual-activation model, which is the model of bilingual cognition the field currently operates under, states that all languages that a bilingual person uses are activated simultaneously.\nHowever, encoding-specificity might have nothing to do with dual-activation at all. While there has always been great debate as to whether bilingualism has an effect on cognitive control processes, a relatively new model of cognitive control (proposed in Green & Abutalebi, 2013) breaks down both cognitive control and bilingualism into more specific categories. This new model, the Adaptive Control Hypothesis, identifies three types of bilingual language use contexts: single language, where each language is used in a distinct context; dual language, where both languages are used in the same context but with different speakers who may speak only one of the bilingual’s languages; and dense code-switching, where both languages are used in the same context but with other bilingual speakers. Bialystok & Craik (2022) built upon the hypothesis, defining which of the 7 control processes received higher demand in each of the 3 interactional contexts of bilingual speakers.\n\n\n\nDemands on control processes in the 3 interactional contexts of bilingual speakers. From (Bialystok & Craik, 2022). Note that + indicates that the context increases the demand on that control process, while = indicates that the context has a neutral effect.\n\n\nUnder the Adaptive Control Hypothesis, it could be theorized that because single-language context use is the context experienced by CLIL students, perhaps Goal Maintenance and Interference Control have the most impact on knowledge encoding and retrieval.\nThough many studies report benefits of CLIL when it comes to language competencies (Dalton-Puffer, 2008), many others report that students in CLIL struggle compared to students in monolingual education, either performing worse overall or needing more time to display the same level of knowledge as their monolingual peers (e.g., Lo and Lo, 2014, Dallinger et al., 2016, Piesche et al., 2016).\nRecent studies in particular focus on LSC, reporting worse performance or longer reaction times when knowledge is retrieved from a different language than it was encoded or acquired in. Wußing et al., 2023, claims that language-switching is directly detrimental to retrieval-based learning. Retrieval-based learning, which has been studied mostly through the “testing-effect”, is a type of learning where a practice test is taken after an initial learning phase to induce the test-taker to “retrieve” the information. Retrieval-based learning consistently increases learning performance compared to the restudy method (where participants restudy the same information after the initial learning phase). Because retrieval-based learning is so effective, it would be concerning for CLIL students if LSC is directly detrimental. Additionally, if different intervals of language-switching had different effects, this might indicate which bilingual use context has the greatest impact on retrieval-based learning. After reading the study, I wanted to use the lens of the Adaptive Control Model to look at the data that was collected.\nAll data in this project comes from Wußing et al., 2023, whose anonymous participant trial data is available for public access and use through the Center for Open Science (aka Open Science Framework, or OSF), a non-profit organization which aims to “increase the openness, integrity, and reproducibility of scientific research and scholarly communication” as an open-source database."
  },
  {
    "objectID": "CLILperm.html#wußing-et-al.-2023",
    "href": "CLILperm.html#wußing-et-al.-2023",
    "title": "Content and Language Integrated-Learning",
    "section": "Wußing et al., 2023",
    "text": "Wußing et al., 2023\nThe study recruited 117 German-English bilingual participants, who were broken down into 6 groups. Half of the participants learned mathematical concepts with a practice-test, and half with a restudy opportunity, creating two within-subject conditions. These groups were then divided into thirds, with one third not switching languages, one switching only for the final test, and another switching between initial and subsequent learning, thus creating three between-subject conditions.\nOn the first day, all participants completed the initial learning phase. There were two texts, both with 10 paragraphs with each paragraph containing a single math concept. Each text was presented twice. After that, the restudy group was shown each concept individually to re-read, while the practice-test group filled in the blank concept name for each paragraph. This phase was also repeated twice.\nAfter 7 days, participants performed a cued recall test and a transfer test. In the cued recall test, participants entered the concept name that matched the presented description. In the transfer test, participants evaluated the correctness of a given statement.\n\n\n\nSchematic overview of the three between-subjects conditions regarding language-switching (Wußing et al., 2023).\n\n\nThe study claims that the results from these tests show the following main effects:\n\nParticipants performed better for the items learned via retrieval-based learning\nParticipants performed worse in conditions with language-switching\nLanguage-switching had a more significant detrimental effect on retrieval-based learning than on restudy-based learning\nLanguage switching had a more significant effect when the switch occurred after the initial learning phase and before subsequent learning, and in fact LSC only occurred with the switching for subsequent learning group\n\nThese claims give us a lot to explore. I have a few permutation tests I want to run on the data. Based on the claims the study makes, here are some Null Hypotheses that I am interested in. While I could use this data to see if retrieval-based learning correlates to better performance, retrieval-based learning is already very well studied and I am more interested in the psycholinguistics questions than the cogsci questions.\nNull Hypothesis: Language switching has no effect on learning performance\nNull Hypothesis: Language switching has no effect on restudy learning performance\nNull Hypothesis: Language switching has no effect on retrieval learning performance\nNull Hypothesis: The timing of a language switch has no impact on learning performance\nLet’s explore the raw data for a moment. The column names were originally all in German, so I used my best judgement when translating, but know that I am not a native German speaker.\n\n\nShow the code\n#renamed all of the German variable names. I couldn't get the Proband:innen one to rename, I think because of the colon, but that's participant ID \n\nTesLaS_DataSet &lt;- TesLaS_DataSet |&gt; \n  rename(\n    trial_number = Nummer,\n    condition = Bedingung,\n    variant = Variante,\n    cued_correct_images = Summe_Korrekt_Abbildungen,\n    cued_correct_statements = Summe_Korrekt_Aussagen,\n    cued_correct_total = Summe_Gesamt,\n    cued_correct_testing = Summe_Korrekt_Testing,\n    cued_correct_restudy = Summe_Korrekt_Restudy,\n    testing_advantage_cued_recall = Testing_Vorteil_CuedRecall,\n    transfer_correct_images = Tra_Summe_Korrekt_Abbildungen,\n    transfer_correct_statements = Tra_Summe_Korrekt_Aussagen,\n    transfer_correct_total = Tra_Summe_Gesamt,\n    transfer_correct_testing = Korrekt_Testing_Transfer,\n    transfer_correct_restudy = Korrekt_Restudy_Transfer,\n    testing_advantage_transfer = Testing_Vorteil_Transfer\n  ) |&gt;\n  mutate(  #renamed the conditions so it's easier to follow later \n  condition = case_when(\n    condition == 1 ~ \"monolingual\",\n    condition == 2 ~ \"switching for final tests\",\n    condition == 3 ~ \"switching for subsequent learning\" )) \n\n\n\nhead(TesLaS_DataSet) #show 6 rows\n\n\n# A tibble: 6 × 16\n  `Proband:innen` trial_number condition   variant cued_correct_images\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;               &lt;dbl&gt;\n1 1onki16                    1 monolingual       1                   8\n2 4ietz13                    2 monolingual       4                   2\n3 3NAER21                    3 monolingual       3                   1\n4 2nees17                    4 monolingual       2                   1\n5 1alki14                    5 monolingual       1                   4\n6 4aner12                    6 monolingual       4                   7\n# ℹ 11 more variables: cued_correct_statements &lt;dbl&gt;, cued_correct_total &lt;dbl&gt;,\n#   cued_correct_testing &lt;dbl&gt;, cued_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_cued_recall &lt;dbl&gt;, transfer_correct_images &lt;dbl&gt;,\n#   transfer_correct_statements &lt;dbl&gt;, transfer_correct_total &lt;dbl&gt;,\n#   transfer_correct_testing &lt;dbl&gt;, transfer_correct_restudy &lt;dbl&gt;,\n#   testing_advantage_transfer &lt;dbl&gt;\n\n\nRecall that there are three conditions: not switching languages, switching only for the final test, and switching between initial and subsequent learning stages. I am most interested in these variables: cued_correct_testing, cued_correct_restudy, transfer_correct_testing, and transfer_correct_restudy. Let’s take a glimpse at the observed means of each of these variables, just to get a feel for the data.\n\n\nShow the code\nTesLaS_means &lt;- TesLaS_DataSet |&gt;\n  group_by(condition) |&gt; \n  summarise(   #collapses it all into the grouped conditions \n    mean_cued_correct_testing = mean(cued_correct_testing, na.rm = TRUE),\n    mean_cued_correct_restudy = mean(cued_correct_restudy, na.rm = TRUE),\n    mean_transfer_correct_testing = mean(transfer_correct_testing, na.rm = TRUE),\n    mean_transfer_correct_restudy = mean(transfer_correct_restudy, na.rm = TRUE),\n    n = n() #number of participants per condition \n  )\n\n\nTesLaS_means #return the tibble, should be 3x6 \n\n\n# A tibble: 3 × 6\n  condition mean_cued_correct_te…¹ mean_cued_correct_re…² mean_transfer_correc…³\n  &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 monoling…                   4.53                   2.5                    7.08\n2 switchin…                   3.34                   2.32                   6.92\n3 switchin…                   2.28                   1.87                   5.82\n# ℹ abbreviated names: ¹​mean_cued_correct_testing, ²​mean_cued_correct_restudy,\n#   ³​mean_transfer_correct_testing\n# ℹ 2 more variables: mean_transfer_correct_restudy &lt;dbl&gt;, n &lt;int&gt;\n\n\nImmediately we can see that for the monolingual condition, the average correct responses for the final tests are higher for the participants who did the practice tests, or who used retrieval learning.\nHowever, when it comes to the group that switched languages halfway through learning, their average correct responses are noticeably lower than the other two conditions when using retrieval learning.\nLet’s run some permutation tests, looking at each of the Null Hypotheses one at a time.\n\nNull Hypothesis 1: Language switching has no effect on learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning. I also grouped together the cued final test and transfer final test, which I will do in every permutation. This is because I am not trying to look at what kind of final test participants perform better on. For this first permutation, I also grouped together the retrieval learning participants and the restudy learning participants, because I am looking at overall learning performace.\n\n\nShow the code\nset.seed(47)\n\nperm1_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm1_stats &lt;- \n  map(1:500, perm1_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm1_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm1_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200\n\n\n\n\nNull Hypothesis 2: Language switching has no effect on restudy learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning and grouped together the cued final test and transfer final test, as in permutation 1. For this permutation, I only looked at the participants who used restudy learning.\n\n\nShow the code\nset.seed(47)\n\nperm2_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_restudy, transfer_correct_restudy) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_restudy + transfer_correct_restudy) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm2_stats &lt;- \n  map(1:500, perm2_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm2_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm2_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1     0.615\n\n\n\n\nNull Hypothesis 3: Language switching has no effect on retrieval learning performance\nFor this permutation, I compared the monolingual condition to the condition which switched for subsequent learning and grouped together the cued final test and transfer final test, as in permutation 1. For this permutation, I only looked at the participants who used retrieval learning.\n\n\nShow the code\nset.seed(47)\n\nperm3_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"monolingual\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing) / 2) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm3_stats &lt;- \n  map(1:500, perm3_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm3_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n#two-sided p value\n\nset.seed(47)\n\n#two-sided p value\nperm3_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1   0.00200\n\n\n\n\nNull Hypothesis 4: The timing of a language switch has no impact on learning performance\nFor this permutation, I compared the condition which switched for the final test to the condition which switched for subsequent learning. This is because I wanted to see if when the language was switched would impact performance. I grouped together the cued final test and transfer final test, and the retrieval learning participants and the restudy learning participants.\n\n\nShow the code\nset.seed(47)\n\nperm4_data &lt;- function(rep, data) {\n  data |&gt;\n    #picked conditions 1 and 3 to compare, I don't think I can do all 3 at once\n    filter(condition == \"switching for final tests\" | condition == \"switching for subsequent learning\") |&gt;\n    select(condition, cued_correct_testing, transfer_correct_testing, cued_correct_restudy, cued_correct_testing) |&gt;\n    \n    #I don't actually care about the results by the different types of final tests, so I'm combining them\n    mutate(combined_testing = (cued_correct_testing + transfer_correct_testing + cued_correct_restudy + cued_correct_testing) / 4) |&gt;\n    \n    select(condition, combined_testing) |&gt;\n    \n    #permute \n    mutate(testing_perm = sample(combined_testing, replace = FALSE)) |&gt;\n    \n    #compute the mean\n    group_by(condition) |&gt;\n    summarize(obs_ave  = mean(combined_testing, na.rm = TRUE), \n              perm_ave = mean(testing_perm, na.rm = TRUE)\n              ) |&gt;\n    \n    arrange(condition) |&gt;\n    \n    #calculate differences\n    summarize(obs_ave_diff  = diff(obs_ave),\n              perm_ave_diff = diff(perm_ave),\n              rep = rep)\n}\n\n\n\n\nShow the code\nset.seed(47)\n\nperm4_stats &lt;- \n  map(1:500, perm4_data, data = TesLaS_DataSet) |&gt; \n  list_rbind() \n\nperm4_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nset.seed(47)\n\n#two-sided p value\nperm4_stats |&gt;\n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff)) + 1) /\n                (n() + 1) #the +1 keeps it from just returning 0 every time \n  )\n\n\n# A tibble: 1 × 1\n  p_val_ave\n      &lt;dbl&gt;\n1    0.0220"
  },
  {
    "objectID": "CLILperm.html#permuted-results",
    "href": "CLILperm.html#permuted-results",
    "title": "Content and Language Integrated-Learning",
    "section": "Permuted Results:",
    "text": "Permuted Results:\nRejected Null Hypothesis: Language switching has no effect on learning performance\nCannot Reject Null Hypothesis: Language switching has no effect on restudy learning performance\nRejected Null Hypothesis: Language switching has no effect on retrieval learning performance\nRejected Null Hypothesis: The timing of a language switch has no impact on learning performance\nKnowing that LSC affect learning performance, particularly retrieval and particularly when the language is changed between subjects, is very useful knowledge for CLIL students. Taking all of your courses in a language you aren’t fluent in is hard enough - knowing what will make it harder is important. I know that as I move forward with French courses, I will keep in mind that I’ll only be making things more difficult for myself if I try to mix English into my studies.\nIt’s also comforting to me, and to anyone else who felt ashamed for backing out of CLIL or who is struggling with CLIL now, to know that the demands that language-switching places on your cognition are very real and capable of disrupting the most tried and true study methods."
  },
  {
    "objectID": "CLILperm.html#references",
    "href": "CLILperm.html#references",
    "title": "Content and Language Integrated-Learning",
    "section": "References",
    "text": "References\nGreen, D. W., & Abutalebi, J. (2013). Language control in bilinguals: The adaptive control hypothesis. Journal of Cognitive Psychology, 25(5), 515–530.https://doi.org/10.1080/20445911.2013.796377\nWußing, M., Grabner, R. H., Sommer, H., & Saalbach, H. (2023). Language-switching and retrieval-based learning: An unfavorable combination. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1198117"
  },
  {
    "objectID": "okcupidethics.html",
    "href": "okcupidethics.html",
    "title": "Project 4",
    "section": "",
    "text": "In 2015, “OkCupid Data for Introductory Statistics and Data Science Courses” by Albert Y. Kim and Adriana Escobedo-Land was published to the academic journal Taylor & Francis online. The dataset was published with the intention of being used for teaching data science and statistics. However, by 2016, the dataset was already appearing in magazines like Fortune, with those who had encountered the dataset claiming the dataset was found to have identifiable information – that is, someone reading the dataset could identify the real person that the OkCupid profile data was scraped from. The dating profile contained extremely sensitive information, ranging from job, salary, age, and height to religion, sexual orientation, and drug use. Being able to identify someone easily from a large, easy to access dataset led to calls for review of the dataset.\nConsent\nNone of the participants consented to be part of the dataset. In the OkCupid privacy policy, users agree to their information being visible to other members on the service, and their data being shared with service providers and operating partner companies. The privacy policy repeatedly insists that privacy and consent are important to the company. It is not unreasonable for a user to read this policy and assume that their personal data would not be scraped for a public database. However, the creators of the dataset were given permission not by the individual participants but rather by the president and founders of the company. The data scraping was thus not illegal. However, Tiffany Xiao &Yifan Ma (2021) points out that legislation about technology and data are outdated, and that while privacy violations are not often illegal, they are still immoral.\nAnonymity\nThe data was found to have identifiable information. The data is from the 2010s, so the data is certainly not old enough to be anonymous. The data contained physical descriptions including height, body type, and race, as well as age, religion, occupation, sex, education, pets, children, and star sign. They also included 10 essay questions which were answered by participants. That combined with the knowledge that all of these people lived within 25 miles of San Francisco – at an exact date which was later removed – and you can definitely identify participants. The data was not at all anonymized until reviews were requested, when essay questions were randomized by row, the exact date was replaced with “from the 2010s”, and random noise was added to the age variable.\nParticipants\nGeneralizing people by sexual orientation is not an algorithm that should be created. That sort of data can be used for little good and substantial harm. To my knowledge the dataset is not currently being used for unintended purposes, but this is not a good precedent to set. Mass amounts of personal information can be easily used for systemic discrimination.\nAvailability\nThis data was made publicly available on GitHub, as well as accessible through Taylor & Francis. You can see and access the data freely online.\nKim, A. Y., & Escobedo-Land, A. (2015). OkCupid Data for Introductory Statistics and Data Science Courses. Journal of Statistics Education, 23(2). https://doi.org/10.1080/10691898.2015.11889737\nPrivacy policy – okcupid. (n.d.-a). https://okcupid-app.zendesk.com/hc/en-us/articles/22780694078491-Privacy-Policy\nXiao, T., & Ma, Y. (2021). A Letter to the Journal of Statistics and Data Science Education — A Call for Review of “OkCupid Data for Introductory Statistics and Data Science Courses” by Albert Y. Kim and Adriana Escobedo-Land. Journal of Statistics and Data Science Education, 29(2), 214–215. https://doi.org/10.1080/26939169.2021.1930812"
  }
]